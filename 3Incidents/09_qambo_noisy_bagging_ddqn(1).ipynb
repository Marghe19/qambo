{"cells":[{"cell_type":"markdown","metadata":{"id":"DLIEalJ1fSoT"},"source":["# Noisy Bagging Double Deep Q Learning - A simple ambulance dispatch point allocation model\n","\n","## Reinforcement learning introduction\n","\n","### RL involves:\n","* Trial and error search\n","* Receiving and maximising reward (often delayed)\n","* Linking state -> action -> reward\n","* Must be able to sense something of their environment\n","* Involves uncertainty in sensing and linking action to reward\n","* Learning -> improved choice of actions over time\n","* All models find a way to balance best predicted action vs. exploration\n","\n","### Elements of RL\n","* *Environment*: all observable and unobservable information relevant to us\n","* *Observation*: sensing the environment\n","* *State*: the perceived (or perceivable) environment\n","* *Agent*: senses environment, decides on action, receives and monitors rewards\n","* *Action*: may be discrete (e.g. turn left) or continuous (accelerator pedal)\n","* *Policy* (how to link state to action; often based on probabilities)\n","* *Reward signal*: aim is to accumulate maximum reward over time\n","* *Value function* of a state: prediction of likely/possible long-term reward\n","* *Q*: prediction of likely/possible long-term reward of an *action*\n","* *Advantage*: The difference in Q between actions in a given state (sums to zero for all actions)\n","* *Model* (optional): a simulation of the environment\n","\n","### Types of model\n","\n","* *Model-based*: have model of environment (e.g. a board game)\n","* *Model-free*: used when environment not fully known\n","* *Policy-based*: identify best policy directly\n","* *Value-based*: estimate value of a decision\n","* *Off-policy*: can learn from historic data from other agent\n","* *On-policy*: requires active learning from current decisions\n","\n","\n","## Duelling Deep Q Networks for Reinforcement Learning\n","\n","Q = The expected future rewards discounted over time. This is what we are trying to maximise.\n","\n","The aim is to teach a network to take the current state observations and recommend the action with greatest Q.\n","\n","Duelling is very similar to Double DQN, except that the policy net splits into two. One component reduces to a single value, which will model the state *value*. The other component models the *advantage*, the difference in Q between different actions (the mean value is subtracted from all values, so that the advtantage always sums to zero). These are aggregated to produce Q for each action.\n","\n","<img src=\"./images/duelling_dqn.png\" width=\"500\"/>\n","\n","Q is learned through the Bellman equation, where the Q of any state and action is the immediate reward achieved + the discounted maximum Q value (the best action taken) of next best action, where gamma is the discount rate.\n","\n","$$Q(s,a)=r + \\gamma.maxQ(s',a')$$\n","\n","## Key DQN components\n","\n","<img src=\"./images/dqn_components.png\" width=\"700\"/>\n","\n","\n","## General method for Q learning:\n","\n","Overall aim is to create a neural network that predicts Q. Improvement comes from improved accuracy in predicting 'current' understood Q, and in revealing more about Q as knowledge is gained (some rewards only discovered after time).\n","\n","<img src=\"./images/dqn_process.png\" width=\"600|\"/>\n","    \n","Target networks are used to stabilise models, and are only updated at intervals. Changes to Q values may lead to changes in closely related states (i.e. states close to the one we are in at the time) and as the network tries to correct for errors it can become unstable and suddenly lose signficiant performance. Target networks (e.g. to assess Q) are updated only infrequently (or gradually), so do not have this instability problem.\n","\n","## Training networks\n","\n","Double DQN contains two networks. This ammendment, from simple DQN, is to decouple training of Q for current state and target Q derived from next state which are closely correlated when comparing input features.\n","\n","The *policy network* is used to select action (action with best predicted Q) when playing the game.\n","\n","When training, the predicted best *action* (best predicted Q) is taken from the *policy network*, but the *policy network* is updated using the predicted Q value of the next state from the *target network* (which is updated from the policy network less frequently). So, when training, the action is selected using Q values from the *policy network*, but the the *policy network* is updated to better predict the Q value of that action from the *target network*. The *policy network* is copied across to the *target network* every *n* steps (e.g. 1000).\n","\n","<img src=\"./images/dqn_training.png\" width=\"700|\"/>\n","\n","## Bagging (Bootstrap Aggregation)\n","\n","Each network is trained from the same memory, but have different starting weights and are trained on different bootstrap samples from that memory. In this example actions are chosen randomly from each of the networks (an alternative could be to take the most common action recommended by the networks, or an average output). This bagging method may also be used to have some measure of uncertainty of action by looking at the distribution of actions recommended from the different nets. Bagging may also be used to aid exploration during stages where networks are providing different suggested action.\n","\n","<img src=\"./images/bagging.png\" width=\"800|\"/>\n","\n","## Noisy layers\n","\n","Noisy layers are an alternative to epsilon-greedy exploration (here, we leave the epsilon-greedy code in the model, but set it to reduce to zero immediately after the period of fully random action choice).\n","\n","For every weight in the layer we have a random value that we draw from the normal distribution. This random value is used to add noise to the output. The parameters for the extent of noise for each weight, sigma, are stored within the layer and get trained as part of the standard back-propogation.\n","\n","A modification to normal nosiy layers is to use layers with ‘factorized gaussian noise’. This reduces the number of random numbers to be sampled (so is less computationally expensive). There are two random vectors, one with the size of the input, and the other with the size of the output. A random matrix is created by calculating the outer product of the two vectors.\n","\n","## References\n","\n","Double DQN:\n","van Hasselt H, Guez A, Silver D. (2015) Deep Reinforcement Learning with Double Q-learning. arXiv:150906461 http://arxiv.org/abs/1509.06461\n","\n","Bagging:\n","Osband I, Blundell C, Pritzel A, et al. (2016) Deep Exploration via Bootstrapped DQN. arXiv:160204621 http://arxiv.org/abs/1602.04621\n","\n","Noisy networks:\n","Fortunato M, Azar MG, Piot B, et al. (2019) Noisy Networks for Exploration. arXiv:170610295 http://arxiv.org/abs/1706.10295\n","\n","Code for the nosiy layers comes from:\n","\n","Lapan, M. (2020). Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition. Packt Publishing.\n"]},{"cell_type":"markdown","metadata":{"id":"Mplp_5JhfSoc"},"source":["## Code structure\n","\n","<img src=\"./images/dqn_program_structure.png\" width=\"700|\"/>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5882,"status":"ok","timestamp":1732607925706,"user":{"displayName":"MARGHERITA MARIA NAPOLITANO","userId":"04703797969186454720"},"user_tz":-60},"id":"Ct0OGVlpeofA","outputId":"d63f342f-ae09-4ec2-97f1-dccdc1fc5778"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: simpy in /usr/local/lib/python3.10/dist-packages (4.1.1)\n"]}],"source":["pip install simpy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22515,"status":"ok","timestamp":1732607948213,"user":{"displayName":"MARGHERITA MARIA NAPOLITANO","userId":"04703797969186454720"},"user_tz":-60},"id":"hzesZEj-eoHh","outputId":"b4f19149-849a-4a74-aab7-aed6b0614131"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"," 0512110799-Napolitano-Margherita-Maria.pdf  'Copia di Use case model.drawio'\n"," 09_category_partition_30_11_2022.gdoc\t      delega-accettazione-eventuale-supplenza-al-ds.doc\n"," 2020.06.10.20127183v2.full.pdf\t\t      DiagrammaNavigazionale2.drawio\n","'ANALISI DEL DATASET.gdoc'\t\t     'Documento senza titolo.gdoc'\n"," biometria\t\t\t\t      IntelligenzaArtificiale\n","'Colab Notebooks'\t\t\t     'L-31 INFORMATICA'\n"]}],"source":["# Import the library to mount Google Drive\n","from google.colab import drive\n","# Mount the Google Drive at /content/drive\n","drive.mount('/content/drive')\n","\n","# Verify by listing the files in the drive\n","!ls /content/drive/My\\ Drive/\n","\n","import sys\n","sys.path.append('/content/drive/MyDrive/IntelligenzaArtificiale/qambo-main')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8gfNEzCEfSod"},"outputs":[],"source":["################################################################################\n","#                           1 Import packages                                  #\n","################################################################################\n","\n","from amboworld.environment import Env\n","\n","import math\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.nn import functional as F\n","\n","# Use a double ended queue (deque) for memory\n","# When memory is full, this will replace the oldest value with the new one\n","from collections import deque\n","\n","# Supress all warnings (e.g. deprecation warnings) for regular use\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XnL4l05MfSoe"},"outputs":[],"source":["################################################################################\n","#                           2 Define model parameters                          #\n","################################################################################\n","\n","# Set whether to display on screen (slows model)\n","DISPLAY_ON_SCREEN = False\n","# Discount rate of future rewards\n","GAMMA = 0.99\n","# Learing rate for neural network\n","LEARNING_RATE = 0.003\n","# Maximum number of game steps (state, action, reward, next state) to keep\n","MEMORY_SIZE = 10000000\n","# Sample batch size for policy network update\n","BATCH_SIZE = 5\n","# Number of game steps to play before starting training (all random actions)\n","REPLAY_START_SIZE = 50000\n","# Number of steps between policy -> target network update\n","SYNC_TARGET_STEPS = 1000\n","# Exploration rate (epsilon) is probability of choosing a random action\n","EXPLORATION_MAX = 1.0\n","EXPLORATION_MIN = 0.0\n","# Reduction in epsilon with each game step\n","EXPLORATION_DECAY = 0.0\n","# Training episodes\n","TRAINING_EPISODES = 50\n","\n","# Set number of parallel networks\n","NUMBER_OF_NETS = 5\n","# Results filename\n","RESULTS_NAME = 'bagging_noisy_ddqn'\n","\n","# SIM PARAMETERS\n","RANDOM_SEED = 42\n","SIM_DURATION = 5000\n","NUMBER_AMBULANCES = 9\n","NUMBER_INCIDENT_POINTS = 3\n","INCIDENT_RADIUS = 2\n","NUMBER_DISPTACH_POINTS = 25\n","AMBOWORLD_SIZE = 50\n","INCIDENT_INTERVAL = 20\n","EPOCHS = 2\n","AMBO_SPEED = 60\n","AMBO_FREE_FROM_HOSPITAL = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wSP91ZvfSof"},"outputs":[],"source":["################################################################################\n","#                      3 Define DQN (Deep Q Network) class                     #\n","#                    (Used for both policy and target nets)                    #\n","################################################################################\n","\n","class DQN(nn.Module):\n","    \"\"\"Deep Q Network. Udes for both policy (action) and target (Q) networks.\"\"\"\n","\n","    def __init__(self, observation_space, action_space):\n","        \"\"\"Constructor method. Set up neural nets.\"\"\"\n","\n","        # nerurones per hidden layer = 2 * max of observations or actions\n","        neurons_per_layer = 2 * max(observation_space, action_space)\n","\n","        # Set starting exploration rate\n","        self.exploration_rate = EXPLORATION_MAX\n","\n","        # Set up action space (choice of possible actions)\n","        self.action_space = action_space\n","\n","        super(DQN, self).__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(observation_space, neurons_per_layer),\n","            nn.ReLU(),\n","            nn.Linear(neurons_per_layer, neurons_per_layer),\n","            nn.ReLU(),\n","            nn.Linear(neurons_per_layer, neurons_per_layer),\n","            nn.ReLU(),\n","            nn.Linear(neurons_per_layer, action_space)\n","        )\n","\n","    def act(self, state):\n","        \"\"\"Act either randomly or by redicting action that gives max Q\"\"\"\n","\n","        # Act randomly if random number < exploration rate\n","        if np.random.rand() < self.exploration_rate:\n","            action = random.randrange(self.action_space)\n","        else:\n","            # Otherwise get predicted Q values of actions\n","            q_values = self.net(torch.FloatTensor(state))\n","            # Get index of action with best Q\n","            action = np.argmax(q_values.detach().numpy()[0])\n","\n","        return action\n","\n","    def forward(self, x):\n","        \"\"\"Forward pass through network\"\"\"\n","        return self.net(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cYoN-9_3fSof"},"outputs":[],"source":["################################################################################\n","#                    4 Define policy net training function                     #\n","################################################################################\n","\n","def optimize(policy_net, target_net, memory):\n","    \"\"\"\n","    Update  model by sampling from memory.\n","    Uses policy network to predict best action (best Q).\n","    Uses target network to provide target of Q for the selected next action.\n","    \"\"\"\n","\n","    # Do not try to train model if memory is less than reqired batch size\n","    if len(memory) < BATCH_SIZE:\n","        return\n","\n","    # Reduce exploration rate (exploration rate is stored in policy net)\n","    policy_net.exploration_rate *= EXPLORATION_DECAY\n","    policy_net.exploration_rate = max(EXPLORATION_MIN,\n","                                      policy_net.exploration_rate)\n","    # Sample a random batch from memory\n","    batch = random.sample(memory, BATCH_SIZE)\n","    for state, action, reward, state_next, terminal in batch:\n","\n","        state_action_values = policy_net(torch.FloatTensor(state))\n","\n","        # Get target Q for policy net update\n","\n","        if not terminal:\n","            # For non-terminal actions get Q from policy net\n","            expected_state_action_values = policy_net(torch.FloatTensor(state))\n","            # Detach next state values from gradients to prevent updates\n","            expected_state_action_values = expected_state_action_values.detach()\n","            # Get next state action with best Q from the policy net (double DQN)\n","            policy_next_state_values = policy_net(torch.FloatTensor(state_next))\n","            policy_next_state_values = policy_next_state_values.detach()\n","            best_action = np.argmax(policy_next_state_values[0].numpy())\n","            # Get target net next state\n","            next_state_action_values = target_net(torch.FloatTensor(state_next))\n","            # Use detach again to prevent target net gradients being updated\n","            next_state_action_values = next_state_action_values.detach()\n","            best_next_q = next_state_action_values[0][best_action].numpy()\n","            updated_q = reward + (GAMMA * best_next_q)\n","            expected_state_action_values[0][action] = updated_q\n","        else:\n","            # For termal actions Q = reward (-1)\n","            expected_state_action_values = policy_net(torch.FloatTensor(state))\n","            # Detach values from gradients to prevent gradient update\n","            expected_state_action_values = expected_state_action_values.detach()\n","            # Set Q for all actions to reward (-1)\n","            expected_state_action_values[0] = reward\n","\n","        # Set net to training mode\n","        policy_net.train()\n","        # Reset net gradients\n","        policy_net.optimizer.zero_grad()\n","        # calculate loss\n","        loss_v = nn.MSELoss()(state_action_values, expected_state_action_values)\n","        # Backpropogate loss\n","        loss_v.backward()\n","        # Update network gradients\n","        policy_net.optimizer.step()\n","\n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PxJbMFahfSof"},"outputs":[],"source":["################################################################################\n","#                            5 Define memory class                             #\n","################################################################################\n","\n","class Memory():\n","    \"\"\"\n","    Replay memory used to train model.\n","    Limited length memory (using deque, double ended queue from collections).\n","      - When memory full deque replaces oldest data with newest.\n","    Holds, state, action, reward, next state, and episode done.\n","    \"\"\"\n","\n","    def __init__(self):\n","        \"\"\"Constructor method to initialise replay memory\"\"\"\n","        self.memory = deque(maxlen=MEMORY_SIZE)\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        \"\"\"state/action/reward/next_state/done\"\"\"\n","        self.memory.append((state, action, reward, next_state, done))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SFswlsjbfSog"},"outputs":[],"source":["################################################################################\n","#                       6  Define results plotting function                    #\n","################################################################################\n","\n","def plot_results(run, exploration, score, mean_call_to_arrival,\n","                 mean_assignment_to_arrival):\n","    \"\"\"Plot and report results at end of run\"\"\"\n","\n","    # Set up chart (ax1 and ax2 share x-axis to combine two plots on one graph)\n","    fig = plt.figure(figsize=(6,6))\n","    ax1 = fig.add_subplot(111)\n","    ax2 = ax1.twinx()\n","\n","    # Plot results\n","    lns1 = ax1.plot(\n","        run, exploration, label='exploration', color='g', linestyle=':')\n","\n","    lns2 = ax2.plot(run, mean_call_to_arrival,\n","             label='call to arrival', color='r')\n","    lns3 = ax2.plot(run, mean_assignment_to_arrival,\n","             label='assignment to arrival', color='b', linestyle='--')\n","\n","    # Get combined legend\n","    lns = lns1 + lns2 + lns3\n","    labs = [l.get_label() for l in lns]\n","    ax1.legend(lns, labs, loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n","\n","    # Set axes\n","    ax1.set_xlabel('run')\n","    ax1.set_ylabel('exploration')\n","    ax2.set_ylabel('Response time')\n","    filename = '/content/drive/MyDrive/IntelligenzaArtificiale/qambo-main/new_output/' + RESULTS_NAME +'.png'\n","    plt.savefig(filename, dpi=300)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XHbsXWJhfSog"},"outputs":[],"source":["################################################################################\n","#                                 7 Main program                               #\n","################################################################################\n","\n","def qambo():\n","    \"\"\"Main program loop\"\"\"\n","\n","    ############################################################################\n","    #                          8 Set up environment                            #\n","    ############################################################################\n","\n","    # Set up game environemnt\n","    sim = Env(\n","        random_seed = RANDOM_SEED,\n","        duration_incidents = SIM_DURATION,\n","        number_ambulances = NUMBER_AMBULANCES,\n","        number_incident_points = NUMBER_INCIDENT_POINTS,\n","        incident_interval = INCIDENT_INTERVAL,\n","        number_epochs = EPOCHS,\n","        number_dispatch_points = NUMBER_DISPTACH_POINTS,\n","        incident_range = INCIDENT_RADIUS,\n","        max_size = AMBOWORLD_SIZE,\n","        ambo_kph = AMBO_SPEED,\n","        ambo_free_from_hospital = AMBO_FREE_FROM_HOSPITAL\n","    )\n","\n","    # Get number of observations returned for state\n","    observation_space = sim.observation_size\n","\n","    # Get number of actions possible\n","    action_space = sim.action_number\n","\n","    ############################################################################\n","    #                    9 Set up policy and target nets                       #\n","    ############################################################################\n","\n","    # Set up policy and target neural nets\n","    policy_nets = [DQN(observation_space, action_space)\n","                   for i in range(NUMBER_OF_NETS)]\n","    target_nets = [DQN(observation_space, action_space)\n","                   for i in range(NUMBER_OF_NETS)]\n","    best_nets = [DQN(observation_space, action_space)\n","                   for i in range(NUMBER_OF_NETS)]\n","\n","    # Set optimizer, copy weights from policy_net to target, and\n","    for i in range(NUMBER_OF_NETS):\n","        # Set optimizer\n","        policy_nets[i].optimizer = optim.Adam(\n","            params=policy_nets[i].parameters(), lr=LEARNING_RATE)\n","        # Copy weights from policy -> target\n","        target_nets[i].load_state_dict(policy_nets[i].state_dict())\n","        # Set target net to eval rather than training mode\n","        target_nets[i].eval()\n","\n","    ############################################################################\n","    #                            10 Set up memory                              #\n","    ############################################################################\n","\n","    # Set up memomry\n","    memory = Memory()\n","\n","    ############################################################################\n","    #                     11 Set up + start training loop                      #\n","    ############################################################################\n","\n","    # Set up run counter and learning loop\n","    run = 0\n","    all_steps = 0\n","    continue_learning = True\n","    best_reward = -np.inf\n","\n","    # Set up list for results\n","    results_run = []\n","    results_exploration = []\n","    results_score = []\n","    results_mean_call_to_arrival = []\n","    results_mean_assignment_to_arrival = []\n","\n","    # Continue repeating games (episodes) until target complete\n","    while continue_learning:\n","\n","        ########################################################################\n","        #                           12 Play episode                            #\n","        ########################################################################\n","\n","        # Increment run (episode) counter\n","        run += 1\n","\n","        ########################################################################\n","        #                             13 Reset game                            #\n","        ########################################################################\n","\n","        # Reset game environment and get first state observations\n","        state = sim.reset()\n","\n","        # Reset total reward and rewards list\n","        total_reward = 0\n","        rewards = []\n","\n","        # Reshape state into 2D array with state obsverations as first 'row'\n","        state = np.reshape(state, [1, observation_space])\n","\n","        # Continue loop until episode complete\n","        while True:\n","\n","            ####################################################################\n","            #                       14 Game episode loop                       #\n","            ####################################################################\n","\n","            ####################################################################\n","            #                       15 Get action                              #\n","            ####################################################################\n","\n","            # Get actions to take (use evalulation mode)\n","            actions = []\n","            for i in range(NUMBER_OF_NETS):\n","                policy_nets[i].eval()\n","                actions.append(policy_nets[i].act(state))\n","\n","            # Randomly choose an action from net actions\n","            random_index = random.randint(0, NUMBER_OF_NETS - 1)\n","            action = actions[random_index]\n","\n","            ####################################################################\n","            #                 16 Play action (get S', R, T)                    #\n","            ####################################################################\n","\n","            # Act\n","            state_next, reward, terminal, info = sim.step(action)\n","            total_reward += reward\n","\n","            # Update trackers\n","            rewards.append(reward)\n","\n","            # Reshape state into 2D array with state observations as first 'row'\n","            state_next = np.reshape(state_next, [1, observation_space])\n","\n","            # Update display if needed\n","            if DISPLAY_ON_SCREEN:\n","                sim.render()\n","\n","            ####################################################################\n","            #                  17 Add S/A/R/S/T to memory                      #\n","            ####################################################################\n","\n","            # Record state, action, reward, new state & terminal\n","            memory.remember(state, action, reward, state_next, terminal)\n","\n","            # Update state\n","            state = state_next\n","\n","            ####################################################################\n","            #                  18 Check for end of episode                     #\n","            ####################################################################\n","\n","            # Actions to take if end of game episode\n","            if terminal:\n","                # Get exploration rate\n","                exploration = policy_nets[0].exploration_rate\n","                # Clear print row content\n","                clear_row = '\\r' + ' ' * 79 + '\\r'\n","                print(clear_row, end='')\n","                print(f'Run: {run}, ', end='')\n","                print(f'Exploration: {exploration: .3f}, ', end='')\n","                average_reward = np.mean(rewards)\n","                print(f'Average reward: {average_reward:4.1f}, ', end='')\n","                mean_assignment_to_arrival = np.mean(info['assignment_to_arrival'])\n","                print(f'Mean assignment to arrival: {mean_assignment_to_arrival:4.1f}, ', end='')\n","                mean_call_to_arrival = np.mean(info['call_to_arrival'])\n","                print(f'Mean call to arrival: {mean_call_to_arrival:4.1f}, ', end='')\n","                demand_met = info['fraction_demand_met']\n","                print(f'Demand met {demand_met:0.3f}')\n","\n","                # Add to results lists\n","                results_run.append(run)\n","                results_exploration.append(exploration)\n","                results_score.append(total_reward)\n","                results_mean_call_to_arrival.append(mean_call_to_arrival)\n","                results_mean_assignment_to_arrival.append(mean_assignment_to_arrival)\n","\n","                # Save model if best reward\n","                total_reward = np.sum(rewards)\n","                if total_reward > best_reward:\n","                    best_reward = total_reward\n","                    # Copy weights to best net\n","                    for i in range(NUMBER_OF_NETS):\n","                        best_nets[i].load_state_dict(policy_nets[i].state_dict())\n","\n","                ################################################################\n","                #             18b Check for end of learning                    #\n","                ################################################################\n","\n","                if run == TRAINING_EPISODES:\n","                    continue_learning = False\n","\n","                # End episode loop\n","                break\n","\n","            ####################################################################\n","            #                        19 Update policy net                      #\n","            ####################################################################\n","\n","            # Avoid training model if memory is not of sufficient length\n","            if len(memory.memory) > REPLAY_START_SIZE:\n","\n","                # Update policy net\n","                for i in range(NUMBER_OF_NETS):\n","                    optimize(policy_nets[i], target_nets[i], memory.memory)\n","\n","                ################################################################\n","                #             20 Update target net periodically                #\n","                ################################################################\n","\n","                # Use load_state_dict method to copy weights from policy net\n","                if all_steps % SYNC_TARGET_STEPS == 0:\n","                    for i in range(NUMBER_OF_NETS):\n","                        target_nets[i].load_state_dict(\n","                            policy_nets[i].state_dict())\n","\n","    ############################################################################\n","    #             21 Learning complete - plot and save results                 #\n","    ############################################################################\n","\n","    # Target reached. Plot results\n","    plot_results(results_run, results_exploration, results_score,\n","                 results_mean_call_to_arrival, results_mean_assignment_to_arrival)\n","\n","    # SAVE RESULTS\n","    run_details = pd.DataFrame()\n","    run_details['run'] = results_run\n","    run_details['exploration '] = results_exploration\n","    run_details['mean_call_to_arrival'] = results_mean_call_to_arrival\n","    run_details['mean_assignment_to_arrival'] = results_mean_assignment_to_arrival\n","    filename = '/content/drive/MyDrive/IntelligenzaArtificiale/qambo-main/new_output/' + RESULTS_NAME + '.csv'\n","    run_details.to_csv(filename, index=False)\n","\n","    ############################################################################\n","    #                             Test best model                              #\n","    ############################################################################\n","\n","    print()\n","    print('Test Model')\n","    print('----------')\n","\n","    for i in range(NUMBER_OF_NETS):\n","        best_nets[i].eval()\n","        best_nets[i].exploration_rate = 0\n","\n","    # Set up results dictionary\n","    results = dict()\n","    results['call_to_arrival'] = []\n","    results['assign_to_arrival'] = []\n","    results['demand_met'] = []\n","\n","    # Replicate model runs\n","    for run in range(30):\n","\n","        # Reset game environment and get first state observations\n","        state = sim.reset()\n","        state = np.reshape(state, [1, observation_space])\n","\n","        # Continue loop until episode complete\n","        while True:\n","            # Get actions to take (use evalulation mode)\n","            actions = []\n","            for i in range(NUMBER_OF_NETS):\n","                actions.append(best_nets[i].act(state))\n","\n","            # Randomly choose an action from net actions\n","            random_index = random.randint(0, NUMBER_OF_NETS - 1)\n","            action = actions[random_index]\n","\n","            # Act\n","            state_next, reward, terminal, info = sim.step(action)\n","            # Reshape state into 2D array with state observations as first 'row'\n","            state_next = np.reshape(state_next, [1, observation_space])\n","            # Update state\n","            state = state_next\n","            if terminal:\n","                print(f'Run: {run}, ', end='')\n","                mean_assignment_to_arrival = np.mean(info['assignment_to_arrival'])\n","                print(f'Mean assignment to arrival: {mean_assignment_to_arrival:4.1f}, ', end='')\n","                mean_call_to_arrival = np.mean(info['call_to_arrival'])\n","                print(f'Mean call to arrival: {mean_call_to_arrival:4.1f}, ', end='')\n","                demand_met = info['fraction_demand_met']\n","                print(f'Demand met: {demand_met:0.3f}')\n","\n","                # Add to results\n","                results['call_to_arrival'].append(mean_call_to_arrival)\n","                results['assign_to_arrival'].append(mean_assignment_to_arrival)\n","                results['demand_met'].append(demand_met)\n","\n","                # End episode loop\n","                break\n","\n","    results = pd.DataFrame(results)\n","    filename = '/content/drive/MyDrive/IntelligenzaArtificiale/qambo-main/new_output/results_' + RESULTS_NAME +'.csv'\n","    results.to_csv(filename, index=False)\n","    print()\n","    print(results.describe())\n","\n","    return run_details"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TDY_zCDxfSoh","outputId":"1d359ee6-dd03-46e3-b404-dc541b2714d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Run: 1, Exploration:  1.000, Average reward: -457.2, Mean assignment to arrival: 18.7, Mean call to arrival: 19.3, Demand met 1.000\n","Run: 2, Exploration:  1.000, Average reward: -462.7, Mean assignment to arrival: 18.8, Mean call to arrival: 19.3, Demand met 1.000\n","Run: 3, Exploration:  1.000, Average reward: -451.3, Mean assignment to arrival: 18.6, Mean call to arrival: 19.1, Demand met 1.000\n","Run: 4, Exploration:  1.000, Average reward: -445.1, Mean assignment to arrival: 18.5, Mean call to arrival: 19.1, Demand met 1.000\n","Run: 5, Exploration:  1.000, Average reward: -455.4, Mean assignment to arrival: 18.8, Mean call to arrival: 19.3, Demand met 0.999\n","Run: 6, Exploration:  1.000, Average reward: -456.7, Mean assignment to arrival: 18.8, Mean call to arrival: 19.3, Demand met 1.000\n","Run: 7, Exploration:  1.000, Average reward: -454.3, Mean assignment to arrival: 18.6, Mean call to arrival: 19.1, Demand met 1.000\n","Run: 8, Exploration:  1.000, Average reward: -460.2, Mean assignment to arrival: 18.8, Mean call to arrival: 19.4, Demand met 1.000\n","Run: 9, Exploration:  1.000, Average reward: -456.5, Mean assignment to arrival: 18.7, Mean call to arrival: 19.2, Demand met 1.000\n","Run: 10, Exploration:  1.000, Average reward: -452.4, Mean assignment to arrival: 18.7, Mean call to arrival: 19.2, Demand met 1.000\n","Run: 11, Exploration:  0.000, Average reward: -290.5, Mean assignment to arrival: 14.9, Mean call to arrival: 15.4, Demand met 1.000\n","Run: 12, Exploration:  0.000, Average reward: -259.5, Mean assignment to arrival: 14.0, Mean call to arrival: 14.5, Demand met 1.000\n","Run: 13, Exploration:  0.000, Average reward: -230.3, Mean assignment to arrival: 13.3, Mean call to arrival: 13.8, Demand met 1.000\n","Run: 14, Exploration:  0.000, Average reward: -211.6, Mean assignment to arrival: 12.7, Mean call to arrival: 13.2, Demand met 1.000\n","Run: 15, Exploration:  0.000, Average reward: -220.0, Mean assignment to arrival: 12.9, Mean call to arrival: 13.4, Demand met 1.000\n","Run: 16, Exploration:  0.000, Average reward: -224.0, Mean assignment to arrival: 13.0, Mean call to arrival: 13.5, Demand met 1.000\n","Run: 17, Exploration:  0.000, Average reward: -233.7, Mean assignment to arrival: 13.4, Mean call to arrival: 13.9, Demand met 1.000\n","Run: 18, Exploration:  0.000, Average reward: -215.7, Mean assignment to arrival: 12.8, Mean call to arrival: 13.3, Demand met 1.000\n","Run: 19, Exploration:  0.000, Average reward: -203.7, Mean assignment to arrival: 12.5, Mean call to arrival: 13.0, Demand met 1.000\n","Run: 20, Exploration:  0.000, Average reward: -182.5, Mean assignment to arrival: 11.9, Mean call to arrival: 12.4, Demand met 1.000\n","Run: 21, Exploration:  0.000, Average reward: -198.5, Mean assignment to arrival: 12.3, Mean call to arrival: 12.8, Demand met 1.000\n","Run: 22, Exploration:  0.000, Average reward: -202.5, Mean assignment to arrival: 12.2, Mean call to arrival: 12.7, Demand met 1.000\n","Run: 23, Exploration:  0.000, Average reward: -209.5, Mean assignment to arrival: 12.6, Mean call to arrival: 13.1, Demand met 1.000\n","Run: 24, Exploration:  0.000, Average reward: -183.9, Mean assignment to arrival: 11.7, Mean call to arrival: 12.2, Demand met 1.000\n","Run: 25, Exploration:  0.000, Average reward: -218.2, Mean assignment to arrival: 12.8, Mean call to arrival: 13.4, Demand met 1.000\n","Run: 26, Exploration:  0.000, Average reward: -193.3, Mean assignment to arrival: 12.1, Mean call to arrival: 12.6, Demand met 1.000\n","Run: 27, Exploration:  0.000, Average reward: -204.1, Mean assignment to arrival: 12.4, Mean call to arrival: 12.9, Demand met 1.000\n","Run: 28, Exploration:  0.000, Average reward: -216.3, Mean assignment to arrival: 12.7, Mean call to arrival: 13.2, Demand met 1.000\n","Run: 29, Exploration:  0.000, Average reward: -220.3, Mean assignment to arrival: 12.9, Mean call to arrival: 13.4, Demand met 1.000\n","Run: 30, Exploration:  0.000, Average reward: -211.0, Mean assignment to arrival: 12.6, Mean call to arrival: 13.1, Demand met 1.000\n","Run: 31, Exploration:  0.000, Average reward: -207.2, Mean assignment to arrival: 12.5, Mean call to arrival: 13.0, Demand met 1.000\n","Run: 32, Exploration:  0.000, Average reward: -206.5, Mean assignment to arrival: 12.5, Mean call to arrival: 13.0, Demand met 1.000\n","Run: 33, Exploration:  0.000, Average reward: -209.6, Mean assignment to arrival: 12.5, Mean call to arrival: 13.0, Demand met 1.000\n","Run: 34, Exploration:  0.000, Average reward: -206.5, Mean assignment to arrival: 12.5, Mean call to arrival: 13.0, Demand met 1.000\n","Run: 35, Exploration:  0.000, Average reward: -222.8, Mean assignment to arrival: 13.0, Mean call to arrival: 13.5, Demand met 1.000\n"]}],"source":["######################## MODEL ENTRY POINT #####################################\n","\n","# Run model and return last run results\n","last_run = qambo()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}